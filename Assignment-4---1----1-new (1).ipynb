{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def search_amazon(product_name):\n",
    "\n",
    "    driver_path = 'path_to_chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "\n",
    "    search_box = driver.find_element_by_name(\"field-keywords\")\n",
    "\n",
    "\n",
    "    search_box.clear()\n",
    "\n",
    "    search_box.send_keys(product_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "    search_results = driver.find_elements_by_css_selector(\".s-result-item\")\n",
    "\n",
    "    for result in search_results:\n",
    "        product_title = result.find_element_by_css_selector(\".a-text-normal\").text\n",
    "        product_link = result.find_element_by_css_selector(\".a-link-normal\").get_attribute(\"href\")\n",
    "        print(f\"Product: {product_title}\\nLink: {product_link}\\n\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_amazon(product_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb547095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for result in soup.find_all('div', class_='s-result-item'):\n",
    "        product_data = {}\n",
    "\n",
    "        try:\n",
    "            product_data['Brand Name'] = result.find('span', class_='a-size-base-plus a-color-base').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Brand Name'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Name of the Product'] = result.find('span', class_='a-text-normal').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Name of the Product'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Price'] = result.find('span', class_='a-price-whole').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Price'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Return/Exchange'] = result.find('div', class_='a-row a-size-small').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Return/Exchange'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Expected Delivery'] = result.find('span', class_='a-text-bold').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Expected Delivery'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Availability'] = result.find('span', class_='a-size-medium a-color-price').text.strip()\n",
    "        except AttributeError:\n",
    "            product_data['Availability'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_data['Product URL'] = result.find('a', class_='a-link-normal')['href']\n",
    "        except (AttributeError, KeyError):\n",
    "            product_data['Product URL'] = '-'\n",
    "\n",
    "        products.append(product_data)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_amazon(search_query, num_pages=3):\n",
    "    driver_path = 'path_to_chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    products = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        url = f'https://www.amazon.in/s?k={search_query}&page={page_num}'\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Allow time for the page to load\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        page_products = scrape_page(page_source)\n",
    "        products.extend(page_products)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(products)\n",
    "\n",
    "    df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    scrape_amazon(search_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    driver_path = 'path_to_chromedriver'  # Replace with the actual path to your Chrome WebDriver executable\n",
    "    download_path = 'image_data/' + keyword\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)\n",
    "\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "\n",
    "    search_box.clear()\n",
    "\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    num_scrolls = num_images // 20\n",
    "    for _ in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    img_tags = soup.find_all('img', class_='rg_i')\n",
    "    img_urls = [img['src'] for img in img_tags if 'src' in img.attrs]\n",
    "\n",
    "    for i, img_url in enumerate(img_urls[:num_images]):\n",
    "        img_name = f\"{keyword}_{i+1}.jpg\"\n",
    "        img_path = os.path.join(download_path, img_name)\n",
    "        with open(img_path, 'wb') as img_file:\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_file.write(img_data)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for '{keyword}'...\")\n",
    "        scrape_images(keyword)\n",
    "        print(f\"Downloaded {keyword} images successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    smartphones = []\n",
    "\n",
    "    for result in soup.find_all('div', class_='_1AtVbE'):\n",
    "        smartphone_data = {}\n",
    "\n",
    "        try:\n",
    "            smartphone_data['Brand Name'] = result.find('div', class_='_4rR01T').text.strip()\n",
    "        except AttributeError:\n",
    "            smartphone_data['Brand Name'] = '-'\n",
    "\n",
    "        try:\n",
    "            smartphone_data['Smartphone Name'] = result.find('a', class_='IRpwTa').text.strip()\n",
    "        except AttributeError:\n",
    "            smartphone_data['Smartphone Name'] = '-'\n",
    "\n",
    "        try:\n",
    "            smartphone_data['Colour'] = result.find('a', class_='IRpwTa').text.strip().split()[-1]\n",
    "        except AttributeError:\n",
    "            smartphone_data['Colour'] = '-'\n",
    "\n",
    "        details = result.find_all('li', class_=\"rgWa7D\")\n",
    "        for detail in details:\n",
    "            text = detail.text.strip()\n",
    "            if \"RAM\" in text:\n",
    "                smartphone_data['RAM'] = text.split('|')[0].strip()\n",
    "            elif \"ROM\" in text:\n",
    "                smartphone_data['Storage(ROM)'] = text.split('|')[0].strip()\n",
    "            elif \"Primary Camera\" in text:\n",
    "                smartphone_data['Primary Camera'] = text.split('|')[0].strip()\n",
    "            elif \"Secondary Camera\" in text:\n",
    "                smartphone_data['Secondary Camera'] = text.split('|')[0].strip()\n",
    "            elif \"Display Size\" in text:\n",
    "                smartphone_data['Display Size'] = text.split('|')[0].strip()\n",
    "            elif \"Battery Capacity\" in text:\n",
    "                smartphone_data['Battery Capacity'] = text.split('|')[0].strip()\n",
    "\n",
    "        try:\n",
    "            smartphone_data['Price'] = result.find('div', class_='_30jeq3').text.strip()\n",
    "        except AttributeError:\n",
    "            smartphone_data['Price'] = '-'\n",
    "\n",
    "        try:\n",
    "            product_url = result.find('a', class_='IRpwTa')['href']\n",
    "            smartphone_data['Product URL'] = 'https://www.flipkart.com' + product_url\n",
    "        except (AttributeError, KeyError):\n",
    "            smartphone_data['Product URL'] = '-'\n",
    "\n",
    "        smartphones.append(smartphone_data)\n",
    "\n",
    "    return smartphones\n",
    "\n",
    "def scrape_flipkart(search_query):\n",
    "    driver_path = 'path_to_chromedriver'  \n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    smartphones = []\n",
    "\n",
    "    url = f'https://www.flipkart.com/search?q={\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "def scrape_coordinates(city):\n",
    " \n",
    "    driver_path = 'path_to_chromedriver'  # Replace with the actual path to your Chrome WebDriver executable\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "        search_box = driver.find_element_by_id(\"searchboxinput\")\n",
    "\n",
    "      \n",
    "        search_box.clear()\n",
    "\n",
    "      \n",
    "        search_box.send_keys(city)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "       \n",
    "        url = driver.current_url\n",
    "        coordinates = extract_coordinates_from_url(url)\n",
    "\n",
    "        return coordinates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "      \n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def extract_coordinates_from_url(url):\n",
    "    try:\n",
    "        lat_start = url.index(\"@\") + 1\n",
    "        lat_end = url.index(\",\", lat_start)\n",
    "        lon_start = lat_end + 1\n",
    "        lon_end = url.index(\",\", lon_start)\n",
    "\n",
    "        latitude = float(url[lat_start:lat_end])\n",
    "        longitude = float(url[lon_start:lon_end])\n",
    "\n",
    "        return {\"Latitude\": latitude, \"Longitude\": longitude}\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name to get its geospatial coordinates: \")\n",
    "    coordinates = scrape_coordinates(city)\n",
    "    if coordinates:\n",
    "        print(f\"Geospatial Coordinates for {city}:\")\n",
    "        print(f\"Latitude: {coordinates['Latitude']}\")\n",
    "        print(f\"Longitude: {coordinates['Longitude']}\")\n",
    "    else:\n",
    "        print(\"Coordinates not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_in_gaming_laptops():\n",
    "\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        laptop_container = soup.find('div', class_='TopNumList')\n",
    "\n",
    "        laptop_names = []\n",
    "        laptop_specs = []\n",
    "\n",
    "        for laptop_entry in laptop_container.find_all('div', class_='Top-Rated-Slider'):\n",
    "        \n",
    "            name = laptop_entry.find('div', class_='TopNumbeTitle').text.strip()\n",
    "            laptop_names.append(name)\n",
    "\n",
    "            specs = []\n",
    "            for spec in laptop_entry.find_all('div', class_='Specs-Wrap'):\n",
    "                spec_name = spec.find('div', class_='Specs').text.strip()\n",
    "                spec_value = spec.find('div', class_='value').text.strip()\n",
    "                specs.append(f'{spec_name}: {spec_value}')\n",
    "            laptop_specs.append(', '.join(specs))\n",
    "\n",
    "        df = pd.DataFrame({'Laptop Name': laptop_names, 'Specifications': laptop_specs})\n",
    "\n",
    "      \n",
    "        df.to_csv('digit_in_gaming_laptops.csv', index=False)\n",
    "\n",
    "        print(\"Scraping completed. Data saved to 'digit_in_gaming_laptops.csv'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Check your internet connection or the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_digit_in_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b94313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        table = soup.find('table', class_='table')\n",
    "\n",
    "        ranks = []\n",
    "        names = []\n",
    "        net_worths = []\n",
    "        ages = []\n",
    "        citizenships = []\n",
    "        sources = []\n",
    "        industries = []\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            net_worth = columns[2].text.strip()\n",
    "            age = columns[3].text.strip()\n",
    "            citizenship = columns[4].text.strip()\n",
    "            source = columns[5].text.strip()\n",
    "            industry = columns[6].text.strip()\n",
    "\n",
    "            ranks.append(rank)\n",
    "            names.append(name)\n",
    "            net_worths.append(net_worth)\n",
    "            ages.append(age)\n",
    "            citizenships.append(citizenship)\n",
    "            sources.append(source)\n",
    "            industries.append(industry)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Rank': ranks,\n",
    "            'Name': names,\n",
    "            'Net Worth': net_worths,\n",
    "            'Age': ages,\n",
    "            'Citizenship': citizenships,\n",
    "            'Source': sources,\n",
    "            'Industry': industries\n",
    "        })\n",
    "\n",
    "    \n",
    "        df.to_csv('forbes_billionaires.csv', index=False)\n",
    "\n",
    "        print(\"Scraping completed. Data saved to 'forbes_billionaires.csv'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Check your internet connection or the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0522227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials_file = 'your_credentials.json'  # Replace with the path to your JSON credentials file\n",
    "\n",
    "api_service_name = 'youtube'\n",
    "api_version = 'v3'\n",
    "scopes = ['https://www.googleapis.com/auth/youtube.force-ssl']\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    credentials_file, scopes=scopes)\n",
    "\n",
    "youtube = googleapiclient.discovery.build(api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "def get_comments(video_id, max_comments=500):\n",
    "    comments = []\n",
    "\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=max_comments,\n",
    "        textFormat='plainText'\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']\n",
    "            comment_text = comment['snippet']['textDisplay']\n",
    "            comment_upvotes = comment['snippet']['likeCount']\n",
    "            comment_time = comment['snippet']['publishedAt']\n",
    "\n",
    "            comments.append({\n",
    "                'Comment': comment_text,\n",
    "                'Upvotes': comment_upvotes,.,ko\n",
    "                'Time Posted': comment_time\n",
    "            })\n",
    "\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID (the part after 'v='): \")\n",
    "    comments = get_comments(video_id)\n",
    "\n",
    "    if comments:\n",
    "        print(f\"Total Comments: {len(comments)}\")\n",
    "        for i, comment in enumerate(comments):\n",
    "            print(f\"Comment {i + 1}:\")\n",
    "            print(f\"Comment: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "            print(f\"Time Posted: {comment['Time Posted']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No comments found for the given video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13136a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = 'https://www.hostelworld.com/s?q=London&country=England'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        hostel_list = []\n",
    "\n",
    "\n",
    "        hostel_elements = soup.find_all('div', class_='fabresult rounded clearfix')\n",
    "\n",
    "        for hostel_element in hostel_elements:\n",
    "            hostel_data = {}\n",
    "\n",
    "            hostel_data['Hostel Name'] = hostel_element.find('h2', class_='title-3').text.strip()\n",
    "            hostel_data['Distance from City Centre'] = hostel_element.find('span', class_='description').text.strip()\n",
    "            hostel_data['Ratings'] = hostel_element.find('div', class_='score orange').text.strip()\n",
    "            hostel_data['Total Reviews'] = hostel_element.find('div', class_='reviews')\n",
    "            hostel_data['Overall Reviews'] = hostel_data['Total Reviews'].find('a').text.strip()\n",
    "            hostel_data['Privates From Price'] = hostel_element.find('span', class_='price').text.strip()\n",
    "            hostel_data['Dorms From Price'] = hostel_element.find('div', class_='prices').find_all('span')[1].text.strip()\n",
    "            hostel_data['Facilities'] = ', '.join([facility.text.strip() for facility in hostel_element.find_all('div', class_='facilities')])\n",
    "\n",
    "            hostel_data['URL'] = 'https://www.hostelworld.com' + hostel_element.find('a', class_='title-2')['href']\n",
    "\n",
    "            property_url = hostel_data['URL']\n",
    "            property_response = requests.get(property_url)\n",
    "            if property_response.status_code == 200:\n",
    "                property_soup = BeautifulSoup(property_response.text, 'html.parser')\n",
    "                property_description = property_soup.find('div', class_='content-style-3').text.strip()\n",
    "                hostel_data['Property Description'] = property_description\n",
    "            else:\n",
    "                hostel_data['Property Description'] = '-'\n",
    "\n",
    "            hostel_list.append(hostel_data)\n",
    "\n",
    "        df = pd.DataFrame(hostel_list)\n",
    "\n",
    "        df.to_csv('hostelworld_london_hostels.csv', index=False)\n",
    "\n",
    "        print(\"Scraping completed. Data saved to 'hostelworld_london_hostels.csv'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Check your internet connection or the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hostels_in_london()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac60cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
