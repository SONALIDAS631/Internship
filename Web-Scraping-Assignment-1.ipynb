{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34d45f4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da77e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "import  requests\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "url = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "soup = BeautifulSoup(url.text, 'html.parser')\n",
    "story = soup.find_all(['h1', 'h2','h3'])\n",
    "for i in story:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape cricket rankings from icc-cricket.com. Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36368710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams Men's Cricket Rankings:\n",
      "-------------------------------------\n",
      "Rank Team                 Matches    Points     Rating    \n",
      "-------------------------------------\n",
      "1    Pakistan\n",
      "PAK         20         2,316      116       \n",
      "2    India\n",
      "IND            36         4,081      113       \n",
      "3    New Zealand\n",
      "NZ       27         2,806      104       \n",
      "4    England\n",
      "ENG          24         2,426      101       \n",
      "5    South Africa\n",
      "SA      19         1,910      101       \n",
      "6    Bangladesh\n",
      "BAN       28         2,661      95        \n",
      "7    Afghanistan\n",
      "AFG      16         1,404      88        \n",
      "8    Sri Lanka\n",
      "SL         32         2,794      87        \n",
      "9    West Indies\n",
      "WI       38         2,582      68        \n",
      "10   Zimbabwe\n",
      "ZIM         30         1,641      55        \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "team_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in team_rows:\n",
    "    columns = row.find_all('td')\n",
    "    teams.append(columns[1].text.strip())\n",
    "    matches.append(columns[2].text)\n",
    "    points.append(columns[3].text)\n",
    "    ratings.append(columns[4].text)\n",
    "\n",
    "print(\"Top 10 ODI Teams Men's Cricket Rankings:\")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"{:<4} {:<20} {:<10} {:<10} {:<10}\".format(\"Rank\", \"Team\", \"Matches\", \"Points\", \"Rating\"))\n",
    "print(\"-------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(\"{:<4} {:<20} {:<10} {:<10} {:<10}\".format(i+1, teams[i], matches[i], points[i], ratings[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ddbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8753002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen Rankings:\n",
      "---------------------------------------\n",
      "Rank Batsman                        Team                 Rating    \n",
      "---------------------------------------\n",
      "1    Rassie van der Dussen          SA                   796 v England, 19/07/2022\n",
      "2    Fakhar Zaman                   PAK                  784 v New Zealand, 29/04/2023\n",
      "3    Imam-ul-Haq                    PAK                  815 v West Indies, 12/06/2022\n",
      "4    Shubman Gill                   IND                  743 v West Indies, 01/08/2023\n",
      "5    Harry Tector                   IRE                  726 v Nepal, 04/07/2023\n",
      "6    David Warner                   AUS                  880 v Pakistan, 26/01/2017\n",
      "7    Quinton de Kock                SA                   813 v Sri Lanka, 10/03/2019\n",
      "8    Virat Kohli                    IND                  911 v England, 12/07/2018\n",
      "9    Steve Smith                    AUS                  752 v Pakistan, 22/01/2017\n",
      "10   Rohit Sharma                   IND                  885 v Sri Lanka, 06/07/2019\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "batsman_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in batsman_rows:\n",
    "    columns = row.find_all('td')\n",
    "    batsman = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    batsmen.append(batsman)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "print(\"Top 10 ODI Batsmen Rankings:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"{:<4} {:<30} {:<20} {:<10}\".format(\"Rank\", \"Batsman\", \"Team\", \"Rating\"))\n",
    "print(\"---------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(\"{:<4} {:<30} {:<20} {:<10}\".format(i+1, batsmen[i], teams[i], ratings[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dcf525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Bowlers Rankings:\n",
      "              Bowler Team                         Rating\n",
      "1     Mitchell Starc  AUS  783 v New Zealand, 29/03/2015\n",
      "2        Rashid Khan  AFG     806 v Pakistan, 21/09/2018\n",
      "3     Mohammed Siraj  IND  736 v New Zealand, 21/01/2023\n",
      "4         Matt Henry   NZ   691 v Bangladesh, 26/03/2021\n",
      "5   Mujeeb Ur Rahman  AFG      712 v Ireland, 24/01/2021\n",
      "6        Trent Boult   NZ    775 v Australia, 11/09/2022\n",
      "7         Adam Zampa  AUS      655 v England, 22/11/2022\n",
      "8     Shaheen Afridi  PAK  688 v West Indies, 10/06/2022\n",
      "9      Kuldeep Yadav  IND  765 v New Zealand, 26/01/2019\n",
      "10   Shakib Al Hasan  BAN     717 v Zimbabwe, 05/11/2009\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "bowlers_data = []\n",
    "\n",
    "bowler_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in bowler_rows:\n",
    "    columns = row.find_all('td')\n",
    "    bowler = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    bowlers_data.append({'Bowler': bowler, 'Team': team, 'Rating': rating})\n",
    "\n",
    "top_bowlers_df = pd.DataFrame(bowlers_data)\n",
    "top_bowlers_df.index += 1  # Adding 1 to index for ranking\n",
    "\n",
    "print(\"Top 10 ODI Bowlers Rankings:\")\n",
    "print(top_bowlers_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cdded",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002 Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5779eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams Rankings:\n",
      "                Team Matches Points Rating\n",
      "1       England\\nENG      31  3,875    125\n",
      "2   South Africa\\nSA      26  3,098    119\n",
      "3         India\\nIND      30  3,039    101\n",
      "4    New Zealand\\nNZ      28  2,688     96\n",
      "5    West Indies\\nWI      29  2,743     95\n",
      "6    Bangladesh\\nBAN      17  1,284     76\n",
      "7      Sri Lanka\\nSL      12    820     68\n",
      "8      Thailand\\nTHA      13    883     68\n",
      "9      Pakistan\\nPAK      27  1,678     62\n",
      "10      Ireland\\nIRE      18    660     37\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams_data = []\n",
    "\n",
    "team_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in team_rows:\n",
    "    columns = row.find_all('td')\n",
    "    team = columns[1].text.strip()\n",
    "    matches = columns[2].text\n",
    "    points = columns[3].text\n",
    "    rating = columns[4].text\n",
    "    \n",
    "    teams_data.append({'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "top_teams_df = pd.DataFrame(teams_data)\n",
    "top_teams_df.index += 1  # Adding 1 to index for ranking\n",
    "\n",
    "print(\"Top 10 Women's ODI Teams Rankings:\")\n",
    "print(top_teams_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "You have to scrape and make data frame\u0002 Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0aacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Batting Players Rankings:\n",
      "                 Player Team                         Rating\n",
      "1   Chamari Athapaththu   SL  758 v New Zealand, 03/07/2023\n",
      "2           Beth Mooney  AUS      776 v England, 12/07/2023\n",
      "3       Laura Wolvaardt   SA    741 v Australia, 22/03/2022\n",
      "4       Smriti Mandhana  IND      797 v England, 28/02/2019\n",
      "5          Alyssa Healy  AUS      785 v England, 03/04/2022\n",
      "6      Harmanpreet Kaur  IND      731 v England, 21/09/2022\n",
      "7          Ellyse Perry  AUS  766 v West Indies, 11/09/2019\n",
      "8           Meg Lanning  AUS  834 v New Zealand, 24/02/2016\n",
      "9       Stafanie Taylor   WI     766 v Pakistan, 07/07/2021\n",
      "10       Tammy Beaumont  ENG        791 v India, 27/06/2021\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "players_data = []\n",
    "\n",
    "player_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in player_rows:\n",
    "    columns = row.find_all('td')\n",
    "    player = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    players_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "\n",
    "top_players_df = pd.DataFrame(players_data)\n",
    "top_players_df.index += 1  # Adding 1 to index for ranking\n",
    "\n",
    "print(\"Top 10 Women's ODI Batting Players Rankings:\")\n",
    "print(top_players_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b79e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33759aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI All-Rounders Rankings:\n",
      "          All-Rounder Team                          Rating\n",
      "1    Ashleigh Gardner  AUS       389 v Ireland, 28/07/2023\n",
      "2     Hayley Matthews   WI       392 v Ireland, 26/06/2023\n",
      "3      Marizanne Kapp   SA   419 v West Indies, 10/09/2021\n",
      "4        Ellyse Perry  AUS   548 v West Indies, 11/09/2019\n",
      "5         Amelia Kerr   NZ   356 v West Indies, 25/09/2022\n",
      "6       Deepti Sharma  IND  397 v South Africa, 09/10/2019\n",
      "7       Jess Jonassen  AUS   308 v West Indies, 11/09/2019\n",
      "8       Sophie Devine   NZ     305 v Australia, 05/10/2020\n",
      "9            Nida Dar  PAK     232 v Australia, 21/01/2023\n",
      "10  Sophie Ecclestone  ENG  217 v South Africa, 31/03/2022\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "allrounders_data = []\n",
    "\n",
    "allrounder_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "for row in allrounder_rows:\n",
    "    columns = row.find_all('td')\n",
    "    allrounder = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    allrounders_data.append({'All-Rounder': allrounder, 'Team': team, 'Rating': rating})\n",
    "\n",
    "top_allrounders_df = pd.DataFrame(allrounders_data)\n",
    "top_allrounders_df.index += 1  # Adding 1 to index for ranking\n",
    "\n",
    "print(\"Top 10 Women's ODI All-Rounders Rankings:\")\n",
    "print(top_allrounders_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002 Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8596fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline\n",
      "0   Nasdaq falls a fourth day in a row, notches lo...\n",
      "1   Can expensive, American-made weapons like F-16...\n",
      "2   Here's why Ukraine wants more big ticket Weste...\n",
      "3   U.S. approves shipments of F-16s to Ukraine in...\n",
      "4   Ukraine to receive F-16 jets from Denmark and ...\n",
      "5   Ukraine sees no hope for F-16s this year; Naft...\n",
      "6   A multitrillion-dollar carbon bubble? Climate ...\n",
      "7   Moving toward low carbon steel is a matter of ...\n",
      "8   We share the same vision as Fortescue Future I...\n",
      "9   Business is war, and we go to war every day: G...\n",
      "10  Offshore wind turbine catches fire off east co...\n",
      "11  Southeast Asia turns to alternative meats as f...\n",
      "12  Southeast Asia moves closer to economic unity ...\n",
      "13  Indonesian Chamber of Commerce and Industry di...\n",
      "14  Stock Exchange of Thailand's president discuss...\n",
      "15  Vietnam's EV ownership will see 'strong growth...\n",
      "16  Chances are you haven’t used A.I. to plan a va...\n",
      "17  $1,850 a day? What it costs to visit the 10 pr...\n",
      "18  Cat-sitting is a whole new travel experience. ...\n",
      "19  A new luxury hotel is opening near Singapore, ...\n",
      "20  A market slaughtering dogs was a top tourist a...\n",
      "21  Harvard gut doctor avoids these 4 foods that c...\n",
      "22         Top 10 best European cities for retirement\n",
      "23  The No. 1 best state to retire in the U.S.—it'...\n",
      "24  Mark Cuban passed on an Uber investment that c...\n",
      "25      The 'world's cheapest home' is on sale for $1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headlines_data = []\n",
    "\n",
    "news_headlines = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "for headline in news_headlines:\n",
    "    title = headline.find('a').text.strip()\n",
    "    headlines_data.append({'Headline': title})\n",
    "\n",
    "headlines_df = pd.DataFrame(headlines_data)\n",
    "print(headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d0b639",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5772\\3149776204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_articles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_previous_sibling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Card-time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mnews_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Headline'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Time'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "news_data = []\n",
    "\n",
    "news_articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "for article in news_articles:\n",
    "    title = article.find('a').text.strip()\n",
    "    time = article.find_previous_sibling('div', class_='Card-time').text.strip()\n",
    "    news_data.append({'Headline': title, 'Time': time})\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa84ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002 News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367a8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline  \\\n",
      "0   Nasdaq falls a fourth day in a row, notches lo...   \n",
      "1   Can expensive, American-made weapons like F-16...   \n",
      "2   Here's why Ukraine wants more big ticket Weste...   \n",
      "3   U.S. approves shipments of F-16s to Ukraine in...   \n",
      "4   Ukraine to receive F-16 jets from Denmark and ...   \n",
      "5   Ukraine sees no hope for F-16s this year; Naft...   \n",
      "6   A multitrillion-dollar carbon bubble? Climate ...   \n",
      "7   Moving toward low carbon steel is a matter of ...   \n",
      "8   We share the same vision as Fortescue Future I...   \n",
      "9   Business is war, and we go to war every day: G...   \n",
      "10  Offshore wind turbine catches fire off east co...   \n",
      "11  Southeast Asia turns to alternative meats as f...   \n",
      "12  Southeast Asia moves closer to economic unity ...   \n",
      "13  Indonesian Chamber of Commerce and Industry di...   \n",
      "14  Stock Exchange of Thailand's president discuss...   \n",
      "15  Vietnam's EV ownership will see 'strong growth...   \n",
      "16  Chances are you haven’t used A.I. to plan a va...   \n",
      "17  $1,850 a day? What it costs to visit the 10 pr...   \n",
      "18  Cat-sitting is a whole new travel experience. ...   \n",
      "19  A new luxury hotel is opening near Singapore, ...   \n",
      "20  A market slaughtering dogs was a top tourist a...   \n",
      "21  Harvard gut doctor avoids these 4 foods that c...   \n",
      "22         Top 10 best European cities for retirement   \n",
      "23  The No. 1 best state to retire in the U.S.—it'...   \n",
      "24  Mark Cuban passed on an Uber investment that c...   \n",
      "25      The 'world's cheapest home' is on sale for $1   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "1   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "2   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "3   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "4   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "5   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "6   https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "7   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "8   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "9   https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "10  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "11  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "12  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "13  https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "14  https://www.cnbc.comhttps://www.cnbc.com/video...  \n",
      "15  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "16  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "17  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "18  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "19  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "20  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "21  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "22  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "23  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "24  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "25  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "news_data = []\n",
    "\n",
    "news_articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "for article in news_articles:\n",
    "    title = article.find('a').text.strip()\n",
    "    link = article.find('a')['href']\n",
    "    full_link = f'https://www.cnbc.com{link}'\n",
    "    news_data.append({'Headline': title, 'News Link': full_link})\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "print(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ef5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8fe5c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "article_titles = soup.find_all('h2', class_='title-text')\n",
    "\n",
    "for title in article_titles:\n",
    "    paper_title = title.text.strip()\n",
    "    articles_data.append({'Paper Title': paper_title})\n",
    "\n",
    "articles_df = pd.DataFrame(articles_data)\n",
    "print(articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame. Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fa81ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "author_sections = soup.find_all('div', class_='author-section')\n",
    "\n",
    "for section in author_sections:\n",
    "    authors = [author.text.strip() for author in section.find_all('span', class_='author-name')]\n",
    "    authors_str = ', '.join(authors)\n",
    "    articles_data.append({'Authors': authors_str})\n",
    "\n",
    "articles_df = pd.DataFrame(articles_data)\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b480f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame. Published Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e22e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "published_dates = soup.find_all('div', class_='article-details')\n",
    "\n",
    "for date in published_dates:\n",
    "    published_date = date.find('span', class_='article-meta-data').text.strip()\n",
    "    articles_data.append({'Published Date': published_date})\n",
    "\n",
    "articles_df = pd.DataFrame(articles_data)\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a251eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame. Paper URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf6285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "article_links = soup.find_all('a', class_='article-content-title')\n",
    "\n",
    "for link in article_links:\n",
    "    paper_url = link['href']\n",
    "    articles_data.append({'Paper URL': paper_url})\n",
    "\n",
    "articles_df = pd.DataFrame(articles_data)\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae60512",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data frame- Restaurant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cb90efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name\n",
      "0                       Station Bar\n",
      "1                             Local\n",
      "2                     My Bar Square\n",
      "3                    Openhouse Cafe\n",
      "4                           Tamasha\n",
      "5                    Warehouse Cafe\n",
      "6                     The G.T. Road\n",
      "7                  Ministry Of Beer\n",
      "8              Connaught Club House\n",
      "9                 The Junkyard Cafe\n",
      "10              Unplugged Courtyard\n",
      "11               Lord of the Drinks\n",
      "12                          Berco's\n",
      "13                      Dasaprakash\n",
      "14              My Bar Headquarters\n",
      "15                        Oh My God\n",
      "16                      38 Barracks\n",
      "17                   Lazeez Affaire\n",
      "18                           Sandoz\n",
      "19                              QBA\n",
      "20  Ardor 2.1 Restaurant and Lounge\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurants_data = []\n",
    "\n",
    "restaurant_names = soup.find_all('div', class_='restnt-info')\n",
    "for restaurant in restaurant_names:\n",
    "    name = restaurant.find('a', class_='restnt-name').text.strip()\n",
    "    restaurants_data.append({'Restaurant Name': name})\n",
    "\n",
    "restaurants_df = pd.DataFrame(restaurants_data)\n",
    "print(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data frame-Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9094d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurants_data = []\n",
    "\n",
    "cuisine_elements = soup.find_all('a', class_='nav-link', href=True)\n",
    "for element in cuisine_elements:\n",
    "    cuisine = element.text.strip()\n",
    "    restaurants_data.append({'Cuisine': cuisine})\n",
    "\n",
    "restaurants_df = pd.DataFrame(restaurants_data)\n",
    "print(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data frame-Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db07444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurants_data = []\n",
    "\n",
    "location_elements = soup.find_all('div', class_='location-text')\n",
    "for element in location_elements:\n",
    "    location = element.text.strip()\n",
    "    restaurants_data.append({'Location': location})\n",
    "\n",
    "restaurants_df = pd.DataFrame(restaurants_data)\n",
    "print(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eebb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data frame-Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0492a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurants_data = []\n",
    "\n",
    "rating_elements = soup.find_all('div', class_='rating-badge')\n",
    "for element in rating_elements:\n",
    "    rating = element.text.strip()\n",
    "    restaurants_data.append({'Ratings': rating})\n",
    "\n",
    "restaurants_df = pd.DataFrame(restaurants_data)\n",
    "print(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data frame-Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c6c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurants_data = []\n",
    "\n",
    "image_elements = soup.find_all('div', class_='restaurant-photo')\n",
    "for element in image_elements:\n",
    "    image_url = element.find('img')['src']\n",
    "    restaurants_data.append({'Image URL': image_url})\n",
    "\n",
    "restaurants_df = pd.DataFrame(restaurants_data)\n",
    "print(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0262c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
